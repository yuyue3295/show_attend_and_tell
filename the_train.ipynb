{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention\n",
    "from tensorflow.contrib.seq2seq import AttentionMechanism\n",
    "from tensorflow.contrib.seq2seq import AttentionWrapper\n",
    "from tensorflow.contrib import seq2seq\n",
    "from tensorflow import layers\n",
    "from tensorflow.layers import dense\n",
    "from tensorflow.nn.rnn_cell import LSTMCell\n",
    "from tensorflow.nn.rnn_cell import DropoutWrapper\n",
    "from tensorflow.nn.rnn_cell import ResidualWrapper\n",
    "from tensorflow.nn.rnn_cell import MultiRNNCell\n",
    "from tqdm import tqdm\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "from tensorflow.python.ops.rnn_cell_impl import LSTMStateTuple\n",
    "from tensorflow.contrib.seq2seq import AttentionWrapperState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![show attend and tell](./imgs/图像标题生成模型结构图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是定义的WordSequence模型，使用WordSequence类统计文本标注语句的词频，并对文本进行编解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSequence(object):\n",
    "    def __init__(self,train_captions,max_len):\n",
    "        self.dict = {'<pad>': 0, '<start>': 1, '<end>': 2,'<unknow>':3}\n",
    "        self.corpus = train_captions\n",
    "        self._word_count()\n",
    "        self.max_len =max_len\n",
    "        \n",
    "        def _word_count(self):\n",
    "            word_dict = {}\n",
    "            for word in self.corpus:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = 0\n",
    "                word_dict[word] = word_dict[word]+1\n",
    "            print(len(word_dict))\n",
    "            word_dict = {k:v for k,v in word_dict.items() if v>=2}\n",
    "            print(len(word_dict))\n",
    "            vocabulary = sorted(word_dict.items(), key=lambda x:-x[1])\n",
    "            for i in vocabulary:\n",
    "                self.dict[i[0]] = len(self.dict)\n",
    "            self.id2word_dict = {value:key for key,value in self.dict.items()}\n",
    "        \n",
    "        \n",
    "    def word2id(self,word_list):\n",
    "        word_list.append('<end>')\n",
    "        ids = []\n",
    "        for word in word_list:\n",
    "            if word in self.dict:\n",
    "                ids.append(self.dict[word])\n",
    "            else:\n",
    "                ids.append(3)\n",
    "        return ids\n",
    "    \n",
    "    def id2word(self,ids):\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            words.append(self.id2word_dict[i])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pares_tfrecords(record):\n",
    "    features = tf.parse_single_example(\n",
    "        record,\n",
    "        features={\n",
    "            'image/height':tf.FixedLenFeature([], tf.int64),\n",
    "            'image/width':tf.FixedLenFeature([], tf.int64),\n",
    "            'image/caption':tf.FixedLenFeature([], tf.string),\n",
    "            'image/caption_len':tf.FixedLenFeature([], tf.int64),\n",
    "            'image/filename':tf.FixedLenFeature([], tf.string),\n",
    "            'image/img':tf.FixedLenFeature([], tf.string)\n",
    "        })\n",
    "    \n",
    "    height = features['image/height']\n",
    "    width = features['image/width']\n",
    "    filename = features['image/filename']\n",
    "    caption = features['image/caption']\n",
    "    caption = tf.string_split([caption]).values\n",
    "    caption = tf.string_to_number(caption)\n",
    "    caption = tf.to_int32(caption)\n",
    "    caption_len = features['image/caption_len']\n",
    "    caption_len = tf.to_int32(caption_len)\n",
    "    img = features['image/img']\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = tf.to_float(img)\n",
    "    img.set_shape(shape=[224,224,3])\n",
    "\n",
    "    #从原始阁像数据解析出图像矩阵，并根据图像尺、f还原图像。\n",
    "    return img,caption,filename,caption_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target_input(img,caption,filename,caption_len):\n",
    "    input_caption = tf.concat([[1],caption[:-1]],axis=0)\n",
    "    label = caption\n",
    "    return img,input_caption,filename,caption_len,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是对tfrecord数据进行解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(batch_size,epochs):\n",
    "    tfrecords_dir = './tfrecords'\n",
    "    files = tf.gfile.Glob(tfrecords_dir + '/*')  # 匹配路径下所有的图片路径，返回一个文件路径列表\n",
    "    dataset = tf.data.TFRecordDataset(files)  # 建立一个dataset\n",
    "    dataset = dataset.map(pares_tfrecords)  #\n",
    "    dataset = dataset.map(make_target_input)\n",
    "    padded_shapes = (\n",
    "        tf.TensorShape([224,224,3]),  # \n",
    "        tf.TensorShape([None]),\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([None])\n",
    "    )\n",
    "    dataset = dataset.repeat(epochs).shuffle(buffer_size=5000)\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    img, caption, filename,caption_len,label= iterator.get_next()\n",
    "# #     caption.set_shape(shape=(batch_size,))\n",
    "#     filename.set_shape(shape=(batch_size,))\n",
    "#     img.set_shape(shape=(batch_size, 224, 224, 3))\n",
    "#     img = tf.to_float(img)\n",
    "    return img, caption, filename,caption_len,label,iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = None\n",
    "with open('./ws.pkl','rb') as f:\n",
    "    ws = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "img, caption, filename,caption_len,label,_ = generate_train_data(10,10)\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init,local_init])\n",
    "    result_img,result_caption,result_filename,result_caption_len,result_label = sess.run([img, caption, filename,caption_len,label])\n",
    "    print(result_caption)\n",
    "    for i in result_caption:\n",
    "        print(ws.id2word(i))\n",
    "    print(result_label)\n",
    "    for j in result_label:\n",
    "        print(ws.id2word(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 使用tensorflow.contrib.slim 模型块儿定义的vgg_19网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_19(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_19',\n",
    "           fc_conv_padding='VALID',\n",
    "           global_pool=False):\n",
    "    with tf.variable_scope(scope, 'vgg_19', [inputs]) as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                            outputs_collections=end_points_collection):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                             scope='dropout6')\n",
    "            net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "            # Convert end_points_collection into a end_point dict.\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "            if global_pool:\n",
    "                net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')\n",
    "                end_points['global_pool'] = net\n",
    "            if num_classes:\n",
    "                net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                               scope='dropout7')\n",
    "            net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                              activation_fn=None,\n",
    "                              normalizer_fn=None,\n",
    "                              scope='fc8')\n",
    "            if spatial_squeeze:\n",
    "                net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "            end_points[sc.name + '/fc8'] = net\n",
    "        return net, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_variables():\n",
    "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='vgg_19')\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = './vgg_model/vgg_19.ckpt'\n",
    "batch_size = 60\n",
    "epochs = 35\n",
    "img, input_caption, filename,caption_len,label,iterator = generate_train_data(batch_size,epochs)\n",
    "net,end_points = vgg_19(img-MEAN_VALUES)\n",
    "img_features = end_points['vgg_19/conv5/conv5_3']\n",
    "print(img_features)\n",
    "img_features = tf.reshape(img_features,shape=[-1,196,512])\n",
    "print(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(inputs, name,training):\n",
    "    return tf.contrib.layers.batch_norm(inputs, decay=0.95, center=True, scale=True, is_training=training, \n",
    "                                        updates_collections=None, scope=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_cell(n_hidden,use_residual,keep_prob_placeholder):\n",
    "        '''\n",
    "        构建一个单独的rnn cell\n",
    "        :param n_hidden: 隐藏层的神经单元数量\n",
    "        :param use_residual: 是否使用residual wrapper\n",
    "        :return:\n",
    "        '''\n",
    "        cell_type = LSTMCell\n",
    "        cell = cell_type(n_hidden)\n",
    "        #使用self.use_dropout 可以避免过拟合，等等。\n",
    "        \n",
    "        cell = DropoutWrapper(\n",
    "            cell,\n",
    "            dtype=tf.float32,\n",
    "            output_keep_prob=keep_prob_placeholder,\n",
    "            seed = 0 #一些层之间操作的随机数\n",
    "            )\n",
    "        #使用ResidualWrapper进行封装可以避免一些梯度消失或者梯度爆炸\n",
    "        if use_residual:\n",
    "            cell = ResidualWrapper(cell)\n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_input_fn(inputs,attention):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    根据attn_input_feeding属性来判断是否在attention计算前进行一次投影的计算\n",
    "    使用注意力机制才会进行的运算\n",
    "    :param inputs:\n",
    "    :param attention:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "#     if not use_residual:\n",
    "#         print(inputs.get_shape,'inputs_shape')\n",
    "#         print(attention.get_shape,'inputs_shape')\n",
    "#         print(tf.concat([inputs,attention],-1),'inputs和attention拼接之后的形状')\n",
    "#         return tf.concat([inputs,attention],-1)\n",
    "\n",
    "    attn_projection = layers.Dense(1024,\n",
    "                                   dtype=tf.float32,\n",
    "                                   use_bias=False,\n",
    "                                   name='attention_cell_input_fn')\n",
    "\n",
    "    '''\n",
    "    这个attn_projection(array_ops.concat([inputs,attention],-1))我的理解就是\n",
    "    layers.Dense(self.hidden_units,\n",
    "                                   dtype=tf.float32,\n",
    "                                   use_bias=False,\n",
    "                                   name='attention_cell_input_fn')(array_ops.concat([inputs,attention],-1))\n",
    "    Dense最终继承了Layer类，Layer中定义了call方法和__call__ 方法，Dense也重写了call方法，__call__方法中调用call方法，call方法中还是起一个全连接层层的作用，__call__\n",
    "    方法中执行流程是：pre process，call，post process\n",
    "    '''\n",
    "    print(inputs,'xixi')\n",
    "    print(attention,'xixi')\n",
    "    t1 = tf.concat([tf.cast(inputs,dtype=tf.float32),tf.cast(attention,dtype=tf.float32)],1)\n",
    "    print(t1,'t1')\n",
    "    return tf.nn.relu(batch_norm(attn_projection(t1),'cell_input_fn',True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_size = 10656\n",
    "hidden_size = 1024\n",
    "max_gradient_norm=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_initializer = tf.random_uniform_initializer(-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义的就是图像生成文本的解码部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('captions') as decoder_scope:\n",
    "    context = batch_norm(img_features,'context',True)\n",
    "    context = tf.nn.relu(context)\n",
    "    attention_mechanism = BahdanauAttention(\n",
    "                num_units=512,\n",
    "                memory=context,\n",
    "                memory_sequence_length=[196]*batch_size\n",
    "            )\n",
    "    print(context,'context')\n",
    "    \n",
    "    cell = MultiRNNCell(\n",
    "            [\n",
    "                build_single_cell(hidden_size,use_residual=True,keep_prob_placeholder = 0.8) for _ in range(1)\n",
    "            ])\n",
    "    cell = AttentionWrapper(\n",
    "            cell=cell,\n",
    "            attention_mechanism=attention_mechanism,\n",
    "            attention_layer_size=hidden_size,\n",
    "            alignment_history=True,#这个是attention的历史信息\n",
    "            cell_input_fn=cell_input_fn,#将attention拼接起来和input拼接起来\n",
    "            name='Attention_Wrapper'\n",
    "        )\n",
    "    \n",
    "    decoder_initial_state = cell.zero_state(\n",
    "            batch_size,tf.float32\n",
    "        )#这里初始化decoder_inital_state\n",
    "    print(decoder_initial_state)\n",
    "\n",
    "#     传递encoder的状态\n",
    "    context_mean = tf.reduce_mean(context, 1)\n",
    "    the_state = tf.nn.relu(batch_norm(dense(context_mean, hidden_size, name='initial_state'),'initial_state',True))\n",
    "    the_memory = tf.nn.relu(batch_norm(dense(context_mean, hidden_size, name='initial_memory'),'initial_memory',True))\n",
    "    the_cell_state_for_init = (LSTMStateTuple(the_memory,the_state),)\n",
    "    print(the_cell_state_for_init)\n",
    "    decoder_initial_state = decoder_initial_state.clone(\n",
    "        cell_state = the_cell_state_for_init\n",
    "    )\n",
    "    \n",
    "    #创建embedding向量\n",
    "    words_embeddings = tf.get_variable(\n",
    "                        name='embeddings',\n",
    "                        shape=(target_vocab_size, 512),\n",
    "                        initializer=e_initializer,\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "    \n",
    "    decoder_output_projection = layers.Dense(\n",
    "                target_vocab_size,\n",
    "                dtype=tf.float32,\n",
    "                use_bias=False,\n",
    "                name='decoder_output_projection'\n",
    "            )\n",
    "    \n",
    "    captions_inputs_embedded = tf.nn.embedding_lookup(\n",
    "                params=words_embeddings,\n",
    "                ids=input_caption\n",
    "            )\n",
    "    \n",
    "    training_helper = seq2seq.TrainingHelper(\n",
    "                inputs=captions_inputs_embedded,#这个是decoder的inputs,不是label\n",
    "                sequence_length=caption_len,#用作输入的解码器长度。\n",
    "                time_major=False,\n",
    "                name='training_helper'\n",
    "            )\n",
    "    \n",
    "    training_decoder = seq2seq.BasicDecoder(\n",
    "                cell=cell,\n",
    "                helper=training_helper,\n",
    "                initial_state=decoder_initial_state\n",
    "            )\n",
    "    \n",
    "    max_decoder_length = tf.reduce_max(\n",
    "                caption_len\n",
    "            )\n",
    "    (\n",
    "            outputs,\n",
    "            final_state,\n",
    "            final_sequence_lengths\n",
    "        ) = seq2seq.dynamic_decode(\n",
    "            decoder=training_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=max_decoder_length,\n",
    "            parallel_iterations=5,\n",
    "            swap_memory=True,\n",
    "            scope=decoder_scope\n",
    "    )\n",
    "    \n",
    "    decoder_logits_train = decoder_output_projection(\n",
    "                outputs.rnn_output\n",
    "            )\n",
    "    \n",
    "    masks = tf.sequence_mask(\n",
    "                lengths=caption_len,\n",
    "                maxlen=max_decoder_length,\n",
    "                dtype=tf.float32,\n",
    "                name='masks'\n",
    "            )\n",
    "    loss = seq2seq.sequence_loss(\n",
    "                logits=decoder_logits_train,\n",
    "                targets=label,#这里应该改成decoder的标签了\n",
    "                weights=masks,# 区分padding位和数据位，这时候需要。\n",
    "                average_across_timesteps=True,\n",
    "                average_across_batch=True\n",
    "            )\n",
    "    print(decoder_logits_train)\n",
    "    print(label)\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(\n",
    "                learning_rate = 0.001\n",
    "            )\n",
    "    \n",
    "    #计算精确度\n",
    "    logits_flatted = tf.reshape(decoder_logits_train,(-1,target_vocab_size))\n",
    "    prediction = tf.argmax(logits_flatted, 1, output_type=tf.int32)\n",
    "    label_flatten = tf.reshape(label, [-1]) \n",
    "    mask_flatten = tf.reshape(masks, [-1])\n",
    "    mask_flatten = tf.cast(mask_flatten,tf.float32)\n",
    "    \n",
    "    correct_prediction = tf.equal(prediction, label_flatten)\n",
    "    print(correct_prediction.get_shape)\n",
    "    print(mask_flatten.get_shape)\n",
    "    correct_prediction_with_mask = tf.multiply(\n",
    "        tf.cast(correct_prediction, tf.float32),\n",
    "        mask_flatten)\n",
    "\n",
    "    mask_sum = tf.reduce_sum(mask_flatten)\n",
    "    accuracy = tf.reduce_sum(correct_prediction_with_mask) / mask_sum\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='captions')\n",
    "trainable_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='captions')\n",
    "global_step = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = tf.gradients(loss,trainable_params)\n",
    "clip_gradients,_ = tf.clip_by_global_norm(gradients,max_gradient_norm)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    with tf.name_scope('train_op'):\n",
    "        train_op = opt.apply_gradients(zip(clip_gradients,trainable_params),\n",
    "                                                global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "saver = tf.train.Saver(get_save_variables())\n",
    "saver1 = tf.train.Saver(save_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([init])\n",
    "    saver.restore(sess=sess,save_path=vgg_model)\n",
    "    saver1.restore(sess=sess,save_path='./mysave_model/captions.ckpt-0')\n",
    "    print(dir(iterator))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "#         sess.run(iterator.initializer())\n",
    "        bar = tqdm(range(529))\n",
    "        try:\n",
    "            for i in bar:\n",
    "                _,loss_val,accuracy_val,global_step_val = sess.run([train_op,loss,accuracy,global_step])\n",
    "                bar.set_description('%s:Epoch, Step: %5d, loss: %3.3f, accuracy: %3.3f'\n",
    "                                                % (str(epoch),global_step_val, loss_val, accuracy_val))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "        saver1.save(sess,save_path='./mysave_model/captions.ckpt-%s' %(str(epoch)))\n",
    "    \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "#     result_img = np.reshape(total_result[0],(224,224,3))\n",
    "#     plt.figure()\n",
    "#     plt.imshow(result_img)\n",
    "#     plt.axis('off')\n",
    "#     print(total_result[0].shape)\n",
    "#     print(' '.join(ws.id2word(total_result[1][0])))\n",
    "#     print(total_result[2])\n",
    "#     print(total_result[3])\n",
    "#     print(total_result[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = None\n",
    "with open('./ws.pkl','rb') as f:\n",
    "    ws = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ws.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[196] * 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init,local_init])\n",
    "    saver.restore(sess=sess,save_path=vgg_model)\n",
    "    total_result = sess.run([img_features])\n",
    "#     result_img = np.reshape(total_result[0],(224,224,3))\n",
    "#     plt.figure()\n",
    "#     plt.imshow(result_img)\n",
    "#     plt.axis('off')\n",
    "#     print(total_result[0].shape)\n",
    "#     print(' '.join(ws.id2word(total_result[1][0])))\n",
    "#     print(total_result[2])\n",
    "#     print(total_result[3])\n",
    "    print(total_result[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
