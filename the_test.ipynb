{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention\n",
    "from tensorflow.contrib.seq2seq import AttentionMechanism\n",
    "from tensorflow.contrib.seq2seq import AttentionWrapper\n",
    "from tensorflow.contrib import seq2seq\n",
    "from tensorflow import layers\n",
    "from tensorflow.layers import dense\n",
    "from tensorflow.nn.rnn_cell import LSTMCell\n",
    "from tensorflow.nn.rnn_cell import DropoutWrapper\n",
    "from tensorflow.nn.rnn_cell import ResidualWrapper\n",
    "from tensorflow.nn.rnn_cell import MultiRNNCell\n",
    "from tqdm import tqdm\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "from tensorflow.python.ops.rnn_cell_impl import LSTMStateTuple\n",
    "from tensorflow.contrib.seq2seq import AttentionWrapperState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSequence(object):\n",
    "    def __init__(self,train_captions,max_len):\n",
    "        self.dict = {'<pad>': 0, '<start>': 1, '<end>': 2,'<unknow>':3}\n",
    "        self.corpus = train_captions\n",
    "        self._word_count()\n",
    "        self.max_len =max_len\n",
    "        \n",
    "        def _word_count(self):\n",
    "            word_dict = {}\n",
    "            for word in self.corpus:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = 0\n",
    "                word_dict[word] = word_dict[word]+1\n",
    "            print(len(word_dict))\n",
    "            word_dict = {k:v for k,v in word_dict.items() if v>=2}\n",
    "            print(len(word_dict))\n",
    "            vocabulary = sorted(word_dict.items(), key=lambda x:-x[1])\n",
    "            for i in vocabulary:\n",
    "                self.dict[i[0]] = len(self.dict)\n",
    "            self.id2word_dict = {value:key for key,value in self.dict.items()}\n",
    "        \n",
    "        \n",
    "    def word2id(self,word_list):\n",
    "        word_list.append('<end>')\n",
    "        ids = []\n",
    "        for word in word_list:\n",
    "            if word in self.dict:\n",
    "                ids.append(self.dict[word])\n",
    "            else:\n",
    "                ids.append(3)\n",
    "        return ids\n",
    "    \n",
    "    def id2word(self,ids):\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            words.append(self.id2word_dict[i])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pares_tfrecords(record):\n",
    "    features = tf.parse_single_example(\n",
    "        record,\n",
    "        features={\n",
    "            'image/height':tf.FixedLenFeature([], tf.int64),\n",
    "            'image/width':tf.FixedLenFeature([], tf.int64),\n",
    "            'image/caption':tf.FixedLenFeature([], tf.string),\n",
    "            'image/caption_len':tf.FixedLenFeature([], tf.int64),\n",
    "            'image/filename':tf.FixedLenFeature([], tf.string),\n",
    "            'image/img':tf.FixedLenFeature([], tf.string)\n",
    "        })\n",
    "    \n",
    "    height = features['image/height']\n",
    "    width = features['image/width']\n",
    "    filename = features['image/filename']\n",
    "    caption = features['image/caption']\n",
    "    caption = tf.string_split([caption]).values\n",
    "    caption = tf.string_to_number(caption)\n",
    "    caption = tf.to_int32(caption)\n",
    "    caption_len = features['image/caption_len']\n",
    "    caption_len = tf.to_int32(caption_len)\n",
    "    img = features['image/img']\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = tf.to_float(img)\n",
    "    img.set_shape(shape=[224,224,3])\n",
    "\n",
    "    #从原始阁像数据解析出图像矩阵，并根据图像尺、f还原图像。\n",
    "    return img,caption,filename,caption_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target_input(img,caption,filename,caption_len):\n",
    "    input_caption = tf.concat([[1],caption[:-1]],axis=0)\n",
    "    label = caption\n",
    "    return img,input_caption,filename,caption_len,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(batch_size,epochs):\n",
    "    tfrecords_dir = './tfrecords'\n",
    "    files = tf.gfile.Glob(tfrecords_dir + '/*')  # 匹配路径下所有的图片路径，返回一个文件路径列表\n",
    "    dataset = tf.data.TFRecordDataset(files)  # 建立一个dataset\n",
    "    dataset = dataset.map(pares_tfrecords)  #\n",
    "    dataset = dataset.map(make_target_input)\n",
    "    padded_shapes = (\n",
    "        tf.TensorShape([224,224,3]),  # \n",
    "        tf.TensorShape([None]),\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([None])\n",
    "    )\n",
    "    dataset = dataset.repeat(epochs).shuffle(buffer_size=5000)\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    img, caption, filename,caption_len,label= iterator.get_next()\n",
    "# #     caption.set_shape(shape=(batch_size,))\n",
    "#     filename.set_shape(shape=(batch_size,))\n",
    "#     img.set_shape(shape=(batch_size, 224, 224, 3))\n",
    "#     img = tf.to_float(img)\n",
    "    return img, caption, filename,caption_len,label,iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_19(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_19',\n",
    "           fc_conv_padding='VALID',\n",
    "           global_pool=False):\n",
    "    with tf.variable_scope(scope, 'vgg_19', [inputs]) as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                            outputs_collections=end_points_collection):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                             scope='dropout6')\n",
    "            net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "            # Convert end_points_collection into a end_point dict.\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "            if global_pool:\n",
    "                net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')\n",
    "                end_points['global_pool'] = net\n",
    "            if num_classes:\n",
    "                net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                               scope='dropout7')\n",
    "            net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                              activation_fn=None,\n",
    "                              normalizer_fn=None,\n",
    "                              scope='fc8')\n",
    "            if spatial_squeeze:\n",
    "                net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "            end_points[sc.name + '/fc8'] = net\n",
    "        return net, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_variables():\n",
    "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='vgg_19')\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    vgg_model = './vgg_model/vgg_19.ckpt'\n",
    "    img = tf.placeholder(shape=(None,224,224,3),dtype=tf.float32)\n",
    "    net,end_points = vgg_19(img-MEAN_VALUES)\n",
    "    img_features = end_points['vgg_19/conv5/conv5_3']\n",
    "    img_features = tf.reshape(img_features,shape=[-1,196,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = './vgg_model/vgg_19.ckpt'\n",
    "batch_size = 2\n",
    "epochs = 35\n",
    "img, input_caption, filename,caption_len,label,iterator = generate_train_data(batch_size,epochs)\n",
    "net,end_points = vgg_19(img-MEAN_VALUES)\n",
    "img_features = end_points['vgg_19/conv5/conv5_3']\n",
    "print(img_features)\n",
    "img_features = tf.reshape(img_features,shape=[-1,196,512])\n",
    "print(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(get_save_variables())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(save_path=vgg_model,sess=sess)\n",
    "    result = sess.run(img_features,feed_dict={img:feed_image})\n",
    "    print(result[0].shape)\n",
    "    plt.imshow(result[0])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(inputs, name,training=False):\n",
    "    return tf.contrib.layers.batch_norm(inputs, decay=0.95, center=True, scale=True, is_training=training, \n",
    "                                        updates_collections=None, scope=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_cell(n_hidden,use_residual=True,keep_prob_placeholder=1):\n",
    "        '''\n",
    "        构建一个单独的rnn cell\n",
    "        :param n_hidden: 隐藏层的神经单元数量\n",
    "        :param use_residual: 是否使用residual wrapper\n",
    "        :return:\n",
    "        '''\n",
    "        cell_type = LSTMCell\n",
    "        cell = cell_type(n_hidden)\n",
    "        #使用self.use_dropout 可以避免过拟合，等等。\n",
    "        \n",
    "        cell = DropoutWrapper(\n",
    "            cell,\n",
    "            dtype=tf.float32,\n",
    "            output_keep_prob=keep_prob_placeholder,\n",
    "            seed = 0 #一些层之间操作的随机数\n",
    "            )\n",
    "        #使用ResidualWrapper进行封装可以避免一些梯度消失或者梯度爆炸\n",
    "        if use_residual:\n",
    "            cell = ResidualWrapper(cell)\n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_input_fn(inputs,attention):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    根据attn_input_feeding属性来判断是否在attention计算前进行一次投影的计算\n",
    "    使用注意力机制才会进行的运算\n",
    "    :param inputs:\n",
    "    :param attention:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "#     if not use_residual:\n",
    "#         print(inputs.get_shape,'inputs_shape')\n",
    "#         print(attention.get_shape,'inputs_shape')\n",
    "#         print(tf.concat([inputs,attention],-1),'inputs和attention拼接之后的形状')\n",
    "#         return tf.concat([inputs,attention],-1)\n",
    "\n",
    "    attn_projection = layers.Dense(1024,\n",
    "                                   dtype=tf.float32,\n",
    "                                   use_bias=False,\n",
    "                                   name='attention_cell_input_fn')\n",
    "\n",
    "    '''\n",
    "    这个attn_projection(array_ops.concat([inputs,attention],-1))我的理解就是\n",
    "    layers.Dense(self.hidden_units,\n",
    "                                   dtype=tf.float32,\n",
    "                                   use_bias=False,\n",
    "                                   name='attention_cell_input_fn')(array_ops.concat([inputs,attention],-1))\n",
    "    Dense最终继承了Layer类，Layer中定义了call方法和__call__ 方法，Dense也重写了call方法，__call__方法中调用call方法，call方法中还是起一个全连接层层的作用，__call__\n",
    "    方法中执行流程是：pre process，call，post process\n",
    "    '''\n",
    "    t1 = tf.concat([tf.cast(inputs,dtype=tf.float32),tf.cast(attention,dtype=tf.float32)],1)\n",
    "    return tf.nn.relu(batch_norm(attn_projection(t1),'cell_input_fn',False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_size = 10656\n",
    "hidden_size = 1024\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_initializer = tf.random_uniform_initializer(-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('captions') as decoder_scope:\n",
    "    context = batch_norm(img_features,'context',False)\n",
    "    context = tf.nn.relu(context)\n",
    "    attention_mechanism = BahdanauAttention(\n",
    "                num_units=512,\n",
    "                memory=context,\n",
    "                memory_sequence_length=[196]*batch_size\n",
    "            )\n",
    "    print(context,'context')\n",
    "    \n",
    "    cell = MultiRNNCell(\n",
    "            [\n",
    "                build_single_cell(hidden_size,use_residual=True,keep_prob_placeholder = 1) for _ in range(1)\n",
    "            ])\n",
    "    cell = AttentionWrapper(\n",
    "            cell=cell,\n",
    "            attention_mechanism=attention_mechanism,\n",
    "            attention_layer_size=hidden_size,\n",
    "            alignment_history=True,#这个是attention的历史信息\n",
    "            cell_input_fn=cell_input_fn,#将attention拼接起来和input拼接起来\n",
    "            name='Attention_Wrapper'\n",
    "        )\n",
    "    \n",
    "    decoder_initial_state = cell.zero_state(\n",
    "            batch_size,tf.float32\n",
    "        )#这里初始化decoder_inital_state\n",
    "    print(decoder_initial_state)\n",
    "\n",
    "#     传递encoder的状态\n",
    "    context_mean = tf.reduce_mean(context, 1)\n",
    "    the_state = tf.nn.relu(batch_norm(dense(context_mean, hidden_size, name='initial_state'),'initial_state',False))\n",
    "    the_memory = tf.nn.relu(batch_norm(dense(context_mean, hidden_size, name='initial_memory'),'initial_memory',False))\n",
    "    the_cell_state_for_init = (LSTMStateTuple(the_memory,the_state),)\n",
    "    print(the_cell_state_for_init)\n",
    "    decoder_initial_state = decoder_initial_state.clone(\n",
    "        cell_state = the_cell_state_for_init\n",
    "    )\n",
    "    \n",
    "    #创建embedding向量\n",
    "    words_embeddings = tf.get_variable(\n",
    "                        name='embeddings',\n",
    "                        shape=(target_vocab_size, 512),\n",
    "                        initializer=e_initializer,\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "    \n",
    "    decoder_output_projection = layers.Dense(\n",
    "                target_vocab_size,\n",
    "                dtype=tf.float32,\n",
    "                use_bias=False,\n",
    "                name='decoder_output_projection'\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def embed_and_input_proj(inputs):\n",
    "        '''\n",
    "        这里根据inputs.name来判断是该返回图像的embed，还是word embed vector\n",
    "        '''\n",
    "\n",
    "        m = tf.nn.embedding_lookup(\n",
    "            words_embeddings,\n",
    "            inputs\n",
    "        )\n",
    "        \n",
    "        return m\n",
    "    start_tokens = tf.tile(\n",
    "                [1],\n",
    "                [batch_size]\n",
    "            )\n",
    "    decoding_helper = seq2seq.GreedyEmbeddingHelper(\n",
    "        start_tokens=start_tokens,#这个是解码开始时，应该首先输入到解码器中去的，但在embed_and_input_proj函数需要替换成embed_img_input\n",
    "        end_token=2,#这里vocab中的结束字符的编码为2\n",
    "        embedding=embed_and_input_proj\n",
    "    )\n",
    "    \n",
    "    inference_decoder  = seq2seq.BasicDecoder(\n",
    "                cell=cell,\n",
    "                helper=decoding_helper,\n",
    "                initial_state=decoder_initial_state,\n",
    "                output_layer=decoder_output_projection\n",
    "            )\n",
    "    \n",
    "    (\n",
    "            outputs,\n",
    "            final_state,\n",
    "            final_sequence_lengths\n",
    "        ) = seq2seq.dynamic_decode(\n",
    "            decoder=inference_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=20,\n",
    "            swap_memory=True,\n",
    "            scope=decoder_scope\n",
    "    )\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='captions')\n",
    "caption_saver = tf.train.Saver(caption_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plt.imread('./data/crop_imgs/2784746.jpg')\n",
    "feed_image = np.reshape(image,newshape=(1,224,224,3))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = None\n",
    "with open('./ws.pkl','rb') as f:\n",
    "    ws = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(save_path=vgg_model,sess=sess)\n",
    "    caption_saver.restore(save_path='./mysave_model/captions.ckpt-31',sess=sess)\n",
    "    result = sess.run([outputs.sample_id,final_sequence_lengths,label,filename])\n",
    "#     print(ws.id2word(result[0]))\n",
    "    index=1\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i,j,m,in zip(result[0],result[2],result[3]):\n",
    "        mm = plt.imread('./data/crop_imgs/'+m.decode())\n",
    "        plt.subplot(batch_size, 1, index)\n",
    "        plt.imshow(mm)\n",
    "        plt.title('predict: '+' '.join(ws.id2word(i)) + '  label: '+' '.join(ws.id2word(j)))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        print()\n",
    "        index = index+1\n",
    "#     print(result[1])\n",
    "#     print(result[0])\n",
    "#     print(result[2])\n",
    "#     print(result[1])\n",
    "#     print(dir(result[2]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
